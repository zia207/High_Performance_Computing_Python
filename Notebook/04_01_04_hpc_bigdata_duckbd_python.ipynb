{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/High_Performance_Computing_Python/blob/main/Notebook/04_01_04_hpc_bigdata_duckbd_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZonSiEqPXnp"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1IFEWet-Aw4DhkkVe1xv_2YYqlvRe9m5_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp73JfVlPM8l"
      },
      "source": [
        "# A Guide to Using DuckDB in Python\n",
        "\n",
        "[DuckDB](https://duckdb.org/) is an open-source, high-performance, in-process analytical database management system designed for data analytics. It’s optimized for columnar data storage and query execution, making it ideal for analytical workloads like those in data science, machine learning, and business intelligence. Unlike traditional databases that run as separate server processes, DuckDB operates within the same process as the application, eliminating the need for a dedicated server and enabling fast, lightweight data processing. It supports SQL queries and integrates seamlessly with various programming languages, including Python, R, and Julia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EILdiDndFB--"
      },
      "source": [
        "DuckDB integrates with Python through the `duckdb` package, which provides an interface to connect to a DuckDB database, execute SQL queries, and work with data in Python. This tutorial introduces DuckDB in Python for handling large datasets that exceed RAM, using the NYC taxi dataset (yellow_tripdata_2023.csv) as an example. DuckDB stores data efficiently and allows direct querying of files without loading them fully into memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQC9ZyqjPM8o"
      },
      "source": [
        "## Key Features of DuckDB\n",
        "\n",
        "- **In-process**: Runs embedded within the host application, reducing overhead.\n",
        "- **Columnar storage**: Optimized for analytical queries with efficient columnar data processing.\n",
        "- **SQL support**: Uses standard SQL for querying, with extensions for advanced analytics.\n",
        "- **High performance**: Leverages modern hardware (e.g., multi-core CPUs, vectorized query execution).\n",
        "- **Lightweight**: No external dependencies, easy to install and use.\n",
        "- **Cross-platform**: Works on Windows, macOS, Linux, and more.\n",
        "- **Integration**: Supports reading/writing from various formats like CSV, Parquet, and JSON."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHKCCnnb-H_l"
      },
      "source": [
        "DuckDB integrates with Python through the `duckdb` package, which provides an interface to connect to a DuckDB database, execute SQL queries, and work with data in Python. This tutorial introduces DuckDB in Python for handling large datasets that exceed RAM, using the NYC taxi dataset (yellow_tripdata_2023.csv) as an example. DuckDB stores data efficiently and allows direct querying of files without loading them fully into memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4npamnZmFIO_"
      },
      "source": [
        "Key Features in Python:\n",
        "\n",
        "* `Integration with Pandas`: DuckDB seamlessly converts between DataFrames and SQL tables.\n",
        "* `Direct File Queries`: Query Parquet, CSV, and JSON without loading into memory.\n",
        "* `Performance`: DuckDB’s vectorized query engine ensures fast execution, even for large datasets.\n",
        "* `Memory Management`: For large datasets, query files directly or use persistent storage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdrWyahm9Wa_"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyzMORaJUCax",
        "outputId": "08a684f1-38c9-4cfe-f6b7-7a5a95353921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jb3r8n-PM8o"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW_AqPKuUCa0",
        "outputId": "03615977-8d39-4978-99a0-64b6e7becbec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.12/dist-packages (1.3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install duckdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi4-VQEgUCa1"
      },
      "outputs": [],
      "source": [
        "# Create a persistent database (stored in a file)\n",
        "con = duckdb.connect('my_duckdb.db')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7uoGgJ2_BZy"
      },
      "source": [
        "## Working with Data\n",
        "\n",
        "DuckDB in Python allows you to:\n",
        "\n",
        "* Execute SQL queries directly.\n",
        "* Register Pandas DataFrames as virtual tables.\n",
        "* DuckDB can ingest data from a wide variety of formats – both on-disk and in-memory. See the data ingestion page for more informatio\n",
        "\n",
        "```\n",
        "import duckdb\n",
        "\n",
        "duckdb.read_csv(\"example.csv\")                # read a CSV file into a Relation\n",
        "duckdb.read_parquet(\"example.parquet\")        # read a Parquet file into a Relation\n",
        "duckdb.read_json(\"example.json\")              # read a JSON file into a Relation\n",
        "\n",
        "duckdb.sql(\"SELECT * FROM 'example.csv'\")     # directly query a CSV file\n",
        "duckdb.sql(\"SELECT * FROM 'example.parquet'\") # directly query a Parquet file\n",
        "duckdb.sql(\"SELECT * FROM 'example.json'\")    # directly query a JSON file\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeJ0IxEjYVcy"
      },
      "source": [
        "## DataFrames\n",
        "\n",
        "DuckDB can directly query Pandas DataFrames, Polars DataFrames and Arrow tables. Note that these are read-only, i.e., editing these tables via INSERT or UPDATE statements is not possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22PkdzRDZB2s"
      },
      "source": [
        "### Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOBKatKyUCa2",
        "outputId": "aa5f9ece-ad10-40a1-a86d-c5f636dd0d38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "┌───────┬─────────┐\n",
              "│  id   │  name   │\n",
              "│ int64 │ varchar │\n",
              "├───────┼─────────┤\n",
              "│     2 │ John    │\n",
              "│     3 │ Charlie │\n",
              "└───────┴─────────┘"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import duckdb\n",
        "import polars as pl\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'id': [1, 2, 3],\n",
        "    'name': ['Alice', 'John', 'Charlie']\n",
        "})\n",
        "duckdb.sql(\"SELECT * FROM df WHERE id > 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V510oHa7Zqyr"
      },
      "source": [
        "### Polars\n",
        "\n",
        "To directly query a Polars DataFrame, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBusks-aUCa3",
        "outputId": "91548a5b-8c64-4f8a-93a5-99f83b17f74d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "┌───────┐\n",
              "│   a   │\n",
              "│ int64 │\n",
              "├───────┤\n",
              "│    42 │\n",
              "└───────┘"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import duckdb\n",
        "import polars as pl\n",
        "\n",
        "polars_df = pl.DataFrame({\"a\": [42]})\n",
        "duckdb.sql(\"SELECT * FROM polars_df\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAWWcBY3Z7v7"
      },
      "source": [
        "### PyArrow\n",
        "To directly query a PyArrow table, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srlj7Kk8UCa3",
        "outputId": "9e1f5582-78ce-4692-eb8c-695ac4d798dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "┌───────┐\n",
              "│   a   │\n",
              "│ int64 │\n",
              "├───────┤\n",
              "│    42 │\n",
              "└───────┘"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import duckdb\n",
        "import pyarrow as pa\n",
        "\n",
        "arrow_table = pa.Table.from_pydict({\"a\": [42]})\n",
        "duckdb.sql(\"SELECT * FROM arrow_table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZjwE4VIaGJc"
      },
      "source": [
        "## Result Conversion\n",
        "DuckDB supports converting query results efficiently to a variety of formats. See the result conversion page for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzif65dyUCa4",
        "outputId": "63c70811-2174-4423-ff7f-f2cef0ba9686"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'42': array([42], dtype=int32)}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import duckdb\n",
        "\n",
        "duckdb.sql(\"SELECT 42\").fetchall()   # Python objects\n",
        "duckdb.sql(\"SELECT 42\").df()         # Pandas DataFrame\n",
        "duckdb.sql(\"SELECT 42\").pl()         # Polars DataFrame\n",
        "duckdb.sql(\"SELECT 42\").arrow()      # Arrow Table\n",
        "duckdb.sql(\"SELECT 42\").fetchnumpy() # NumPy Arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xPDKffhahKD"
      },
      "source": [
        "## Writing Data to Disk\n",
        "\n",
        "DuckDB supports writing Relation objects directly to disk in a variety of formats. The COPY statement can be used to write data to disk using SQL as an alternative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4tZLj5jUCa5"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "\n",
        "duckdb.sql(\"SELECT 42\").write_parquet(\"out.parquet\") # Write to a Parquet file\n",
        "duckdb.sql(\"SELECT 42\").write_csv(\"out.csv\")         # Write to a CSV file\n",
        "duckdb.sql(\"COPY (SELECT 42) TO 'out.parquet'\")      # Copy to a Parquet file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpYkysLN_9qs"
      },
      "source": [
        "## Reading/Writing External Files\n",
        "DuckDB supports direct reading and writing of files like `CSV` and `Parquet`. You can query these files as if they were tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxJEu5HyUCa5",
        "outputId": "673cacb8-c32b-4aac-f228-5faec6625703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    LocationID Borough           Zone service_zone\n",
            "0            2  Queens    Jamaica Bay    Boro Zone\n",
            "1            7  Queens        Astoria    Boro Zone\n",
            "2            8  Queens   Astoria Park    Boro Zone\n",
            "3            9  Queens     Auburndale    Boro Zone\n",
            "4           10  Queens   Baisley Park    Boro Zone\n",
            "..         ...     ...            ...          ...\n",
            "64         226  Queens      Sunnyside    Boro Zone\n",
            "65         252  Queens     Whitestone    Boro Zone\n",
            "66         253  Queens  Willets Point    Boro Zone\n",
            "67         258  Queens      Woodhaven    Boro Zone\n",
            "68         260  Queens       Woodside    Boro Zone\n",
            "\n",
            "[69 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "\n",
        "# Query a CSV file directly\n",
        "queens = duckdb.sql(\"SELECT * FROM '/content/drive/MyDrive/Data/CSV_files/taxi_zone_lookup.csv' WHERE Borough = 'Queens'\").fetchdf()\n",
        "print(queens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6064oLSPM8q"
      },
      "source": [
        "## Connecting to DuckDB\n",
        "\n",
        "DuckDB can operate in two modes in Python:\n",
        "\n",
        "* `In-memory`: Data is stored in memory (default).\n",
        "* `Persistent`: Data is stored on disk for persistence across sessions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ANs8UGUaxy3"
      },
      "source": [
        "### Using an In-Memory Database\n",
        "\n",
        "When using DuckDB through `duckdb.sql()`, it operates on an in-memory database, i.e., no tables are persisted on disk. Invoking the duckdb.connect() method without arguments returns a connection, which also uses an in-memory database:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSjouRwsUCa6",
        "outputId": "1255bc0d-2a18-4a50-dd53-7ee97b5f0312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(1, 'Alice'), (2, 'Bob')]\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "\n",
        "# In-memory database\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Create and query a table\n",
        "con.execute(\"CREATE TABLE example (id INTEGER, name VARCHAR);\")\n",
        "con.execute(\"INSERT INTO example VALUES (1, 'Alice'), (2, 'Bob');\")\n",
        "\n",
        "result = con.sql(\"SELECT * FROM example\").fetchall()\n",
        "print(result)\n",
        "\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZF3JjnBba7r"
      },
      "source": [
        "### Persistent Storage\n",
        "The duckdb.connect(dbname) creates a connection to a persistent database. Any data written to that connection will be persisted, and can be reloaded by reconnecting to the same file, both from Python and from other DuckDB clients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab12e4LZUCa7",
        "outputId": "858cca6b-79fe-46aa-f8bf-75b3aade9b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "┌───────┐\n",
            "│   i   │\n",
            "│ int32 │\n",
            "├───────┤\n",
            "│    42 │\n",
            "└───────┘\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "\n",
        "# create a connection to a file called 'file.db'\n",
        "con = duckdb.connect(\"file.db\")\n",
        "# create a table and load data into it\n",
        "con.sql(\"CREATE TABLE test (i INTEGER)\")\n",
        "con.sql(\"INSERT INTO test VALUES (42)\")\n",
        "# query the table\n",
        "con.table(\"test\").show()\n",
        "# explicitly close the connection\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTjw9OtPUCa7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuGUGbIoPM8r"
      },
      "source": [
        "## Executing SQL Queries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpUoSGHzuLEv"
      },
      "source": [
        "SQL queries can be executed using the duckdb.sql function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x0K9it8UCa7",
        "outputId": "c30da40d-ef43-4408-f0cb-370cf04072a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "┌───────┐\n",
            "│  42   │\n",
            "│ int32 │\n",
            "├───────┤\n",
            "│    42 │\n",
            "└───────┘\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "\n",
        "duckdb.sql(\"SELECT 42\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSYCz_ghuVHT"
      },
      "source": [
        "The result can be converted to various formats using the result conversion functions. For example, the `fetchall` method can be used to convert the result to Python objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcOWPLhjUCa8",
        "outputId": "49593d5d-9357-425c-f6ea-d26d4f9349de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(42,)]\n"
          ]
        }
      ],
      "source": [
        "results = duckdb.sql(\"SELECT 42\").fetchall()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1DuGFdvvBCk"
      },
      "source": [
        "You can use df to convert the result to a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVVdHROrUCa8",
        "outputId": "d690bef9-2cc7-497a-fdeb-abf9aedd987b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   42\n",
            "0  42\n"
          ]
        }
      ],
      "source": [
        "results = duckdb.sql(\"SELECT 42\").df()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy0YHME5vNpd"
      },
      "source": [
        "By default, a global in-memory connection will be used. Any data stored in files will be lost after shutting down the program. A connection to a persistent database can be created using the connect function.\n",
        "\n",
        "After connecting, SQL queries can be executed using the sql command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4zmM-EHUCbI",
        "outputId": "0df4f10b-8231-447a-9099-789ae436f0a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id   name\n",
            "0   1  Alice\n",
            "1   2    Bob\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "\n",
        "con = duckdb.connect()\n",
        "\n",
        "# Example: Create a table and insert data\n",
        "con.execute(\"CREATE TABLE example (id INTEGER, name VARCHAR);\")\n",
        "con.execute(\"INSERT INTO example VALUES (1, 'Alice'), (2, 'Bob');\")\n",
        "\n",
        "# Query the table\n",
        "result = con.execute(\"SELECT * FROM example\").fetchdf()\n",
        "print(result)\n",
        "#    id   name\n",
        "# 0   1  Alice\n",
        "# 1   2    Bob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1Nm4saQUCbI"
      },
      "outputs": [],
      "source": [
        "# Export query results to a Parquet file\n",
        "duckdb.sql(\"COPY (SELECT * FROM queens) TO 'queens.parquet' (FORMAT PARQUET);\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS3r3swxxG2s"
      },
      "source": [
        "### SQL on Pandas\n",
        "Pandas DataFrames stored in local variables can be queried as if they are regular tables within DuckDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hYtsgJBUCbJ",
        "outputId": "3ded01e0-8958-40a0-8523-a6d58d9d15c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    a\n",
            "0  42\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "import pandas\n",
        "\n",
        "# Create a Pandas dataframe\n",
        "my_df = pandas.DataFrame.from_dict({'a': [42]})\n",
        "\n",
        "# query the Pandas DataFrame \"my_df\"\n",
        "# Note: duckdb.sql connects to the default in-memory database connection\n",
        "results = duckdb.sql(\"SELECT * FROM my_df\").df()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b56qh5jqxVoG"
      },
      "source": [
        "### SQL on Apache Arrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ysxZXtxUCbJ",
        "outputId": "422e5c91-fbca-4495-b52e-970b5bee5a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pyarrow.Table\n",
            "i: int64\n",
            "j: string\n",
            "----\n",
            "i: [[2]]\n",
            "j: [[\"two\"]]\n"
          ]
        }
      ],
      "source": [
        "# Apache Arrow Tables\n",
        "import duckdb\n",
        "import pyarrow as pa\n",
        "\n",
        "# connect to an in-memory database\n",
        "con = duckdb.connect()\n",
        "\n",
        "my_arrow_table = pa.Table.from_pydict({'i': [1, 2, 3, 4],\n",
        "                                       'j': [\"one\", \"two\", \"three\", \"four\"]})\n",
        "\n",
        "# query the Apache Arrow Table \"my_arrow_table\" and return as an Arrow Table\n",
        "results = con.execute(\"SELECT * FROM my_arrow_table WHERE i = 2\").arrow()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azK04DnNyIGX"
      },
      "source": [
        "## Integration with Ibis\n",
        "\n",
        "[lbis](https://ibis-project.org/) is a Python dataframe library that supports 20+ backends, with DuckDB as the default. Ibis with DuckDB provides a Pythonic interface for SQL with great performance.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf6QIjUvzNHP"
      },
      "source": [
        "### Installation\n",
        "You can pip install Ibis with the DuckDB backend:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE9WAxbOUCbJ"
      },
      "outputs": [],
      "source": [
        "!pip install 'ibis-framework[duckdb]'\n",
        "!pip install pins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Ia7aZ4zx8u"
      },
      "source": [
        "### Create a Database File\n",
        "\n",
        "Ibis can work with several file types, but at its core, it connects to existing databases and interacts with the data there. You can get started with your own DuckDB databases or create a new one with example data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHyjiplGUCbK",
        "outputId": "55c5b242-3467-4a02-c4f3-d1c621a007b5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatabaseTable: penguins.main.penguins\n",
              "  species           string\n",
              "  island            string\n",
              "  bill_length_mm    float64\n",
              "  bill_depth_mm     float64\n",
              "  flipper_length_mm int64\n",
              "  body_mass_g       int64\n",
              "  sex               string\n",
              "  year              int64\n",
              "</pre>\n"
            ],
            "text/plain": [
              "DatabaseTable: penguins.main.penguins\n",
              "  species           string\n",
              "  island            string\n",
              "  bill_length_mm    float64\n",
              "  bill_depth_mm     float64\n",
              "  flipper_length_mm int64\n",
              "  body_mass_g       int64\n",
              "  sex               string\n",
              "  year              int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import ibis\n",
        "\n",
        "con = ibis.connect(\"duckdb://penguins.ddb\")\n",
        "con.create_table(\n",
        "    \"penguins\", ibis.examples.penguins.fetch().to_pyarrow(), overwrite = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yojl9mO5UCbK",
        "outputId": "d83ce716-40a4-4f16-ca2e-7333b51d8722"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['penguins']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reconnect to the persisted database (dropping temp tables)\n",
        "con = ibis.connect(\"duckdb://penguins.ddb\")\n",
        "con.list_tables()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y5Gh16G04Dt"
      },
      "source": [
        "There's one table, called penguins. We can ask Ibis to give us an object that we can interact with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJkK1ccdUCbK",
        "outputId": "5c81bec2-7cf9-479a-cf10-b1eaae35797c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatabaseTable: penguins\n",
              "  species           string\n",
              "  island            string\n",
              "  bill_length_mm    float64\n",
              "  bill_depth_mm     float64\n",
              "  flipper_length_mm int64\n",
              "  body_mass_g       int64\n",
              "  sex               string\n",
              "  year              int64\n",
              "</pre>\n"
            ],
            "text/plain": [
              "DatabaseTable: penguins\n",
              "  species           string\n",
              "  island            string\n",
              "  bill_length_mm    float64\n",
              "  bill_depth_mm     float64\n",
              "  flipper_length_mm int64\n",
              "  body_mass_g       int64\n",
              "  sex               string\n",
              "  year              int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "penguins = con.table(\"penguins\")\n",
        "penguins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TowAY6MR1CPy"
      },
      "source": [
        "We can call `head` and then `to_pandas` to get the first few rows of the table as a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHsN-n9pUCbL",
        "outputId": "91586e07-0ed8-43d8-a154-80d2cc58ddf4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"penguins\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"species\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Adelie\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"island\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Torgersen\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill_length_mm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5491933384829646,\n        \"min\": 36.7,\n        \"max\": 40.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          39.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bill_depth_mm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8266397845091503,\n        \"min\": 17.4,\n        \"max\": 19.3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          17.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flipper_length_mm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.4485140407177015,\n        \"min\": 181.0,\n        \"max\": 195.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          186.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body_mass_g\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 259.40637360455634,\n        \"min\": 3250.0,\n        \"max\": 3800.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3800.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"female\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2007,\n        \"max\": 2007,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2007\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-9470f845-6868-4e65-8bc1-55fb300aa39f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>species</th>\n",
              "      <th>island</th>\n",
              "      <th>bill_length_mm</th>\n",
              "      <th>bill_depth_mm</th>\n",
              "      <th>flipper_length_mm</th>\n",
              "      <th>body_mass_g</th>\n",
              "      <th>sex</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>39.1</td>\n",
              "      <td>18.7</td>\n",
              "      <td>181.0</td>\n",
              "      <td>3750.0</td>\n",
              "      <td>male</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>39.5</td>\n",
              "      <td>17.4</td>\n",
              "      <td>186.0</td>\n",
              "      <td>3800.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>40.3</td>\n",
              "      <td>18.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>3250.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Adelie</td>\n",
              "      <td>Torgersen</td>\n",
              "      <td>36.7</td>\n",
              "      <td>19.3</td>\n",
              "      <td>193.0</td>\n",
              "      <td>3450.0</td>\n",
              "      <td>female</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9470f845-6868-4e65-8bc1-55fb300aa39f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9470f845-6868-4e65-8bc1-55fb300aa39f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9470f845-6868-4e65-8bc1-55fb300aa39f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f0613f16-300e-4124-8f58-7697ed893a73\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0613f16-300e-4124-8f58-7697ed893a73')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f0613f16-300e-4124-8f58-7697ed893a73 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
              "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
              "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
              "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
              "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
              "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
              "\n",
              "   body_mass_g     sex  year  \n",
              "0       3750.0    male  2007  \n",
              "1       3800.0  female  2007  \n",
              "2       3250.0  female  2007  \n",
              "3          NaN    None  2007  \n",
              "4       3450.0  female  2007  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "penguins.head().to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW0dyZjn5Un_"
      },
      "source": [
        "## Integration with Polars\n",
        "\n",
        "[Polars](https://github.com/pola-rs/polars) is a DataFrames library built in Rust with bindings for Python and Node.js. It uses Apache Arrow's columnar format as its memory model. DuckDB can read Polars DataFrames and convert query results to Polars DataFrames. It does this internally using the efficient Apache Arrow integration. Note that the pyarrow library must be installed for the integration to work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87uyTAXF5ilG"
      },
      "source": [
        "### Installtion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UUmdDWRUCbL"
      },
      "outputs": [],
      "source": [
        "!pip install -U duckdb 'polars[pyarrow]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoSq1ZzY54QM"
      },
      "source": [
        "### Polars to DuckDB\n",
        "\n",
        "DuckDB can natively query Polars DataFrames by referring to the name of Polars DataFrames as they exist in the current scope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUv47Sj1UCbM",
        "outputId": "ec703451-868d-443e-9297-3383c4c9da03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "┌───────┬─────────┬───────┬─────────┐\n",
            "│   A   │ fruits  │   B   │  cars   │\n",
            "│ int64 │ varchar │ int64 │ varchar │\n",
            "├───────┼─────────┼───────┼─────────┤\n",
            "│     1 │ banana  │     5 │ beetle  │\n",
            "│     2 │ banana  │     4 │ audi    │\n",
            "│     3 │ apple   │     3 │ beetle  │\n",
            "│     4 │ apple   │     2 │ beetle  │\n",
            "│     5 │ banana  │     1 │ beetle  │\n",
            "└───────┴─────────┴───────┴─────────┘\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "import polars as pl\n",
        "\n",
        "df = pl.DataFrame(\n",
        "    {\n",
        "        \"A\": [1, 2, 3, 4, 5],\n",
        "        \"fruits\": [\"banana\", \"banana\", \"apple\", \"apple\", \"banana\"],\n",
        "        \"B\": [5, 4, 3, 2, 1],\n",
        "        \"cars\": [\"beetle\", \"audi\", \"beetle\", \"beetle\", \"beetle\"],\n",
        "    }\n",
        ")\n",
        "duckdb.sql(\"SELECT * FROM df\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KedNFJff6F7w"
      },
      "source": [
        "### DuckDB to Polars\n",
        "DuckDB can output results as Polars DataFrames using the .pl() result-conversion method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyA3_h_mUCbM",
        "outputId": "d8459212-6358-40d6-b13e-f34aa9016c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape: (3, 2)\n",
            "┌─────┬────────┐\n",
            "│ id  ┆ fruit  │\n",
            "│ --- ┆ ---    │\n",
            "│ i32 ┆ str    │\n",
            "╞═════╪════════╡\n",
            "│ 1   ┆ banana │\n",
            "│ 2   ┆ apple  │\n",
            "│ 3   ┆ mango  │\n",
            "└─────┴────────┘\n"
          ]
        }
      ],
      "source": [
        "df = duckdb.sql(\"\"\"\n",
        "    SELECT 1 AS id, 'banana' AS fruit\n",
        "    UNION ALL\n",
        "    SELECT 2, 'apple'\n",
        "    UNION ALL\n",
        "    SELECT 3, 'mango'\"\"\"\n",
        ").pl()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akJrex0LD5ZH"
      },
      "source": [
        "## Large Data Processing with DuckBD\n",
        "\n",
        "This section of the  tutorial uses the NYC Yellow Taxi Trip Data for January 2023 (~3 million rows, 47 MB Parquet file). DuckDB can query this file directly without loading it into memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbbn-sbPETew"
      },
      "source": [
        "### Data\n",
        "\n",
        "he dataset is in Parquet format, which DuckDB can query directly without loading into memory. For reproducibility:\n",
        "- Download the January 2023 data from:\n",
        "[https://d37ci07v2hxiua.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet](https://d37ci07v2hxiua.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet) (about 47 MB, ~3 million rows).\n",
        "- We saved  it to a local folder, e.g., `/home/zia207/Dropbox/WebSites/R_Website/Quarto_Projects/R_Beginner/Data/yellow_tripdata_2023-01.parquet`.\n",
        "- We'll also use the Taxi Zone Lookup CSV for joins: Download from [https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv](https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv).\n",
        "\n",
        "The dataset includes columns like `VendorID`, `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `passenger_count`, `trip_distance`, `PULocationID`, `DOLocationID`, `fare_amount`, `total_amount`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TD1vtHHUCbM"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "import pandas as pd\n",
        "\n",
        "# Set data folder\n",
        "DATA_FOLDER = \"/content/drive/MyDrive/Data/CSV_files/\"\n",
        "PARQUET_FILE = DATA_FOLDER + \"yellow_tripdata_2023-01.parquet\"\n",
        "ZONE_FILE = DATA_FOLDER + \"taxi_zone_lookup.csv\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q44x6TQBE677"
      },
      "source": [
        "### Connect and Query Parquet Directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI7ZGAsvUCbN",
        "outputId": "cb6756ff-1b26-49f8-d4d7-14351ec39941",
        "colab": {
          "referenced_widgets": [
            "4a903bd847254f70806d30646391e131"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a903bd847254f70806d30646391e131",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "0         2  2023-01-01 00:32:10   2023-01-01 00:40:36              1.0   \n",
            "1         2  2023-01-01 00:55:08   2023-01-01 01:01:27              1.0   \n",
            "2         2  2023-01-01 00:25:04   2023-01-01 00:37:49              1.0   \n",
            "3         1  2023-01-01 00:03:48   2023-01-01 00:13:25              0.0   \n",
            "4         2  2023-01-01 00:10:29   2023-01-01 00:21:19              1.0   \n",
            "5         2  2023-01-01 00:50:34   2023-01-01 01:02:52              1.0   \n",
            "6         2  2023-01-01 00:09:22   2023-01-01 00:19:49              1.0   \n",
            "7         2  2023-01-01 00:27:12   2023-01-01 00:49:56              1.0   \n",
            "8         2  2023-01-01 00:21:44   2023-01-01 00:36:40              1.0   \n",
            "9         2  2023-01-01 00:39:42   2023-01-01 00:50:36              1.0   \n",
            "\n",
            "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
            "0           0.97         1.0                  N           161           141   \n",
            "1           1.10         1.0                  N            43           237   \n",
            "2           2.51         1.0                  N            48           238   \n",
            "3           1.90         1.0                  N           138             7   \n",
            "4           1.43         1.0                  N           107            79   \n",
            "5           1.84         1.0                  N           161           137   \n",
            "6           1.66         1.0                  N           239           143   \n",
            "7          11.70         1.0                  N           142           200   \n",
            "8           2.95         1.0                  N           164           236   \n",
            "9           3.01         1.0                  N           141           107   \n",
            "\n",
            "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
            "0             2          9.3   1.00      0.5        0.00           0.0   \n",
            "1             1          7.9   1.00      0.5        4.00           0.0   \n",
            "2             1         14.9   1.00      0.5       15.00           0.0   \n",
            "3             1         12.1   7.25      0.5        0.00           0.0   \n",
            "4             1         11.4   1.00      0.5        3.28           0.0   \n",
            "5             1         12.8   1.00      0.5       10.00           0.0   \n",
            "6             1         12.1   1.00      0.5        3.42           0.0   \n",
            "7             1         45.7   1.00      0.5       10.74           3.0   \n",
            "8             1         17.7   1.00      0.5        5.68           0.0   \n",
            "9             2         14.9   1.00      0.5        0.00           0.0   \n",
            "\n",
            "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
            "0                    1.0         14.30                   2.5         0.00  \n",
            "1                    1.0         16.90                   2.5         0.00  \n",
            "2                    1.0         34.90                   2.5         0.00  \n",
            "3                    1.0         20.85                   0.0         1.25  \n",
            "4                    1.0         19.68                   2.5         0.00  \n",
            "5                    1.0         27.80                   2.5         0.00  \n",
            "6                    1.0         20.52                   2.5         0.00  \n",
            "7                    1.0         64.44                   2.5         0.00  \n",
            "8                    1.0         28.38                   2.5         0.00  \n",
            "9                    1.0         19.90                   2.5         0.00  \n",
            "              column_name column_type null   key default extra\n",
            "0                VendorID      BIGINT  YES  None    None  None\n",
            "1    tpep_pickup_datetime   TIMESTAMP  YES  None    None  None\n",
            "2   tpep_dropoff_datetime   TIMESTAMP  YES  None    None  None\n",
            "3         passenger_count      DOUBLE  YES  None    None  None\n",
            "4           trip_distance      DOUBLE  YES  None    None  None\n",
            "5              RatecodeID      DOUBLE  YES  None    None  None\n",
            "6      store_and_fwd_flag     VARCHAR  YES  None    None  None\n",
            "7            PULocationID      BIGINT  YES  None    None  None\n",
            "8            DOLocationID      BIGINT  YES  None    None  None\n",
            "9            payment_type      BIGINT  YES  None    None  None\n",
            "10            fare_amount      DOUBLE  YES  None    None  None\n",
            "11                  extra      DOUBLE  YES  None    None  None\n",
            "12                mta_tax      DOUBLE  YES  None    None  None\n",
            "13             tip_amount      DOUBLE  YES  None    None  None\n",
            "14           tolls_amount      DOUBLE  YES  None    None  None\n",
            "15  improvement_surcharge      DOUBLE  YES  None    None  None\n",
            "16           total_amount      DOUBLE  YES  None    None  None\n",
            "17   congestion_surcharge      DOUBLE  YES  None    None  None\n",
            "18            airport_fee      DOUBLE  YES  None    None  None\n",
            "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
            "0         1  2023-01-01 00:43:37   2023-01-01 01:17:18              4.0   \n",
            "1         2  2023-01-01 00:09:29   2023-01-01 00:29:23              2.0   \n",
            "2         1  2023-01-01 00:03:36   2023-01-01 00:09:36              3.0   \n",
            "3         1  2023-01-01 00:15:23   2023-01-01 00:29:41              2.0   \n",
            "4         1  2023-01-01 00:21:49   2023-01-01 00:29:15              4.0   \n",
            "5         1  2023-01-01 00:52:06   2023-01-01 01:02:18              2.0   \n",
            "6         1  2023-01-01 00:22:24   2023-01-01 00:35:11              2.0   \n",
            "7         1  2023-01-01 00:31:30   2023-01-01 00:46:51              2.0   \n",
            "8         2  2023-01-01 00:27:16   2023-01-01 00:59:51              4.0   \n",
            "9         2  2023-01-01 00:39:14   2023-01-01 00:47:26              2.0   \n",
            "\n",
            "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
            "0           7.30         1.0                  N            79           264   \n",
            "1          11.43         1.0                  N           138            33   \n",
            "2           1.20         1.0                  N           237           239   \n",
            "3           2.50         1.0                  N           143           229   \n",
            "4           0.80         1.0                  N           163           161   \n",
            "5           1.70         1.0                  N           161           164   \n",
            "6           2.30         1.0                  N            43           262   \n",
            "7           4.10         1.0                  N           170            75   \n",
            "8           2.02         1.0                  N           163           186   \n",
            "9           2.41         1.0                  N           233           263   \n",
            "\n",
            "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
            "0             1         33.8    3.5      0.5        7.75           0.0   \n",
            "1             1         44.3    6.0      0.5       13.26           0.0   \n",
            "2             2          8.6    3.5      0.5        0.00           0.0   \n",
            "3             2         15.6    3.5      0.5        0.00           0.0   \n",
            "4             4          8.6    3.5      0.5        0.00           0.0   \n",
            "5             4         11.4    3.5      0.5        0.00           0.0   \n",
            "6             1         13.5    3.5      0.5        3.70           0.0   \n",
            "7             2         19.1    3.5      0.5        0.00           0.0   \n",
            "8             2         26.1    1.0      0.5        0.00           0.0   \n",
            "9             1         12.1    1.0      0.5        3.42           0.0   \n",
            "\n",
            "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
            "0                    1.0         46.55                   2.5         0.00  \n",
            "1                    1.0         66.31                   0.0         1.25  \n",
            "2                    1.0         13.60                   2.5         0.00  \n",
            "3                    1.0         20.60                   2.5         0.00  \n",
            "4                    1.0         13.60                   2.5         0.00  \n",
            "5                    1.0         16.40                   2.5         0.00  \n",
            "6                    1.0         22.20                   2.5         0.00  \n",
            "7                    1.0         24.10                   2.5         0.00  \n",
            "8                    1.0         31.10                   2.5         0.00  \n",
            "9                    1.0         20.52                   2.5         0.00  \n"
          ]
        }
      ],
      "source": [
        "# Query Parquet file directly (no connection needed!)\n",
        "preview = duckdb.sql(f\"SELECT * FROM '{PARQUET_FILE}' LIMIT 10\").fetchdf()\n",
        "print(preview)\n",
        "\n",
        "# Get schema\n",
        "schema = duckdb.sql(f\"DESCRIBE '{PARQUET_FILE}'\").fetchdf()\n",
        "print(schema)\n",
        "\n",
        "# Basic filtering\n",
        "filtered = duckdb.sql(f\"\"\"\n",
        "    SELECT * FROM '{PARQUET_FILE}'\n",
        "    WHERE passenger_count > 1\n",
        "    LIMIT 10\n",
        "\"\"\").fetchdf()\n",
        "print(filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNdm1KiVFXib"
      },
      "source": [
        "### Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2MzpTS-UCbN",
        "outputId": "4b2f7fe8-d93c-4272-a6e3-40f850244ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   avg_distance  total_trips\n",
            "0      3.847342      3066766\n"
          ]
        }
      ],
      "source": [
        "# Average trip distance and total trips\n",
        "agg = duckdb.sql(f\"\"\"\n",
        "    SELECT\n",
        "        AVG(trip_distance) AS avg_distance,\n",
        "        COUNT(*) AS total_trips\n",
        "    FROM '{PARQUET_FILE}'\n",
        "\"\"\").fetchdf()\n",
        "print(agg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muobK_0YFeCv"
      },
      "source": [
        "### Group By"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIeF5ebFUCbO",
        "outputId": "871f2713-849b-4e78-e9ce-273abf6e7043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   payment_type  total_revenue   avg_tip\n",
            "0             1   6.824062e+07  4.170799\n",
            "1             2   1.226006e+07  0.001675\n",
            "2             0   2.090131e+06  3.733109\n",
            "3             3   1.893647e+05  0.029469\n",
            "4             4   8.502335e+04  0.051490\n"
          ]
        }
      ],
      "source": [
        "group_result = duckdb.sql(f\"\"\"\n",
        "    SELECT\n",
        "        payment_type,\n",
        "        SUM(total_amount) AS total_revenue,\n",
        "        AVG(tip_amount) AS avg_tip\n",
        "    FROM '{PARQUET_FILE}'\n",
        "    GROUP BY payment_type\n",
        "    ORDER BY total_revenue DESC\n",
        "\"\"\").fetchdf()\n",
        "print(group_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LGs5bNtFmsz"
      },
      "source": [
        "### Register Files as Tables (for reuse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAm7pxA6UCbO",
        "outputId": "e93e4bf7-0bcb-44c4-94cb-4433f0995db0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<duckdb.duckdb.DuckDBPyConnection at 0x7af0c8315e30>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "con = duckdb.connect()\n",
        "\n",
        "# Register Parquet and CSV as views\n",
        "con.execute(f\"CREATE VIEW taxi_data AS SELECT * FROM '{PARQUET_FILE}'\")\n",
        "con.execute(f\"CREATE VIEW zones AS SELECT * FROM '{ZONE_FILE}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ1Mewg3Frmv"
      },
      "source": [
        "### Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvtV76dIUCbO",
        "outputId": "17b46be0-8111-45bc-caa3-1fab43cc0b81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   VendorID  trip_distance  total_amount pickup_borough\n",
            "0         2          11.70         64.44      Manhattan\n",
            "1         2          11.43         66.31         Queens\n",
            "2         1          17.80         95.15         Queens\n",
            "3         2          11.11         61.86         Queens\n",
            "4         2          16.02         64.85         Queens\n",
            "5         1          11.30         64.20      Manhattan\n",
            "6         2          11.19         72.90      Manhattan\n",
            "7         2          11.03         68.50         Queens\n",
            "8         2          13.54         78.41         Queens\n",
            "9         1          19.20         91.25         Queens\n"
          ]
        }
      ],
      "source": [
        "# Join with zone lookup\n",
        "joined = con.execute(\"\"\"\n",
        "    SELECT\n",
        "        t.VendorID,\n",
        "        t.trip_distance,\n",
        "        t.total_amount,\n",
        "        z.Borough AS pickup_borough\n",
        "    FROM taxi_data t\n",
        "    LEFT JOIN zones z ON t.PULocationID = z.LocationID\n",
        "    WHERE t.trip_distance > 10\n",
        "    LIMIT 10\n",
        "\"\"\").fetchdf()\n",
        "print(joined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-2ruIwxFy2E"
      },
      "source": [
        "### Window Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mms8cM78UCbP",
        "outputId": "4f617792-3704-4ad6-ab75-08c64bb7a278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    payment_type  trip_distance  distance_rank\n",
            "0              2       62359.52              1\n",
            "1              2        9674.67              2\n",
            "2              2        6508.23              3\n",
            "3              2        5056.82              4\n",
            "4              2        2034.31              5\n",
            "5              2         605.28              6\n",
            "6              2         204.10              7\n",
            "7              2         177.88              8\n",
            "8              2         159.85              9\n",
            "9              2         147.83             10\n",
            "10             2         127.98             11\n",
            "11             2         127.98             11\n",
            "12             2         111.60             13\n",
            "13             2         103.80             14\n",
            "14             2         100.46             15\n",
            "15             2          93.05             16\n",
            "16             2          90.63             17\n",
            "17             2          88.83             18\n",
            "18             2          87.53             19\n",
            "19             2          85.94             20\n"
          ]
        }
      ],
      "source": [
        "window = con.execute(\"\"\"\n",
        "    SELECT\n",
        "        payment_type,\n",
        "        trip_distance,\n",
        "        RANK() OVER (PARTITION BY payment_type ORDER BY trip_distance DESC) AS distance_rank\n",
        "    FROM taxi_data\n",
        "    LIMIT 20\n",
        "\"\"\").fetchdf()\n",
        "print(window)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9LM0YGIF94w"
      },
      "source": [
        "### Query Multiple Remote Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abNOeRX0UCbP",
        "outputId": "326bda79-f607-437d-ec70-6c59192ce8ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   month    trips\n",
            "0      1  3066727\n",
            "1      2  2913910\n",
            "2      3       42\n",
            "3     10       11\n",
            "4     12       31\n"
          ]
        }
      ],
      "source": [
        "# Enable HTTPFS for remote files\n",
        "con = duckdb.connect()\n",
        "con.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
        "\n",
        "# Query two months of taxi data from S3\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "    EXTRACT(MONTH FROM tpep_pickup_datetime) AS month,\n",
        "    COUNT(*) AS trips\n",
        "FROM read_parquet([\n",
        "    'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet',\n",
        "    'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet'\n",
        "])\n",
        "GROUP BY month\n",
        "ORDER BY month;\n",
        "\"\"\"\n",
        "\n",
        "result = con.execute(query).fetchdf()\n",
        "print(result)\n",
        "\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzVBPEcoGLV4"
      },
      "source": [
        "### Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBgPEGX9UCbP",
        "outputId": "e4b49754-9272-4272-f4ff-1be6411eb00c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<duckdb.duckdb.DuckDBPyConnection at 0x7af0c8315e30>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "con.execute(f\"\"\"\n",
        "    COPY (\n",
        "        SELECT * FROM taxi_data WHERE trip_distance > 5 LIMIT 1000\n",
        "    ) TO '{DATA_FOLDER}output.csv' (FORMAT CSV, HEADER)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXFHpiOEGT_v"
      },
      "source": [
        "### Close connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot6G1IQsUCbP"
      },
      "outputs": [],
      "source": [
        "con.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd59Io1Z8BHl"
      },
      "source": [
        "## Relational API on Pandas\n",
        "DuckDB offers a relational API that can be used to chain together query operations. These are lazily evaluated so that DuckDB can optimize their execution. These operators can act on Pandas DataFrames, DuckDB tables or views (which can point to any underlying storage format that DuckDB can read, such as CSV or Parquet files, etc.). Here we show a simple example of reading from a Pandas DataFrame and returning a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee9p0dHkUCbQ"
      },
      "outputs": [],
      "source": [
        "# Set data folder (adjust as needed for your environment)\n",
        "DATA_FOLDER = \"/content/drive/MyDrive/Data/CSV_files/\"\n",
        "PARQUET_FILE = DATA_FOLDER + \"yellow_tripdata_2023-01.parquet\"\n",
        "ZONE_FILE = DATA_FOLDER + \"taxi_zone_lookup.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW_BoaDhAtsE"
      },
      "source": [
        "### Import and Connect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgzY7lKaUCbQ"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "import pandas as pd\n",
        "\n",
        "# Create a persistent or in-memory connection\n",
        "con = duckdb.connect()  # in-memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyXpWvgaAsi_"
      },
      "source": [
        "### Basic Filtering & Aggregation on Parquetmory connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuKtQ975UCbQ",
        "outputId": "8d789e4f-30d5-480e-e986-446289fbf82d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   avg_distance  total_trips\n",
            "0      3.905751      3020904\n"
          ]
        }
      ],
      "source": [
        "# Lazily read Parquet file and compute average trip distance\n",
        "avg_trip = (\n",
        "    con.read_parquet(PARQUET_FILE)\n",
        "    .filter(\"trip_distance > 0\")\n",
        "    .aggregate(\"AVG(trip_distance) AS avg_distance, COUNT(*) AS total_trips\")\n",
        "    .fetchdf()\n",
        ")\n",
        "\n",
        "print(avg_trip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGczNh1FBJdk"
      },
      "source": [
        "### Group By Payment Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9iN7OvoUCbR",
        "outputId": "10a04df7-1233-402f-ee52-bf588423e2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method NDFrame.head of    payment_type  total_revenue   avg_tip\n",
            "0             1   6.824062e+07  4.170799\n",
            "1             2   1.226006e+07  0.001675\n",
            "2             0   2.090131e+06  3.733109\n",
            "3             3   1.893647e+05  0.029469\n",
            "4             4   8.502335e+04  0.051490>\n"
          ]
        }
      ],
      "source": [
        "payment_summary = (\n",
        "    con.read_parquet(PARQUET_FILE)\n",
        "    .aggregate(\"\"\"\n",
        "        payment_type,\n",
        "        SUM(total_amount) AS total_revenue,\n",
        "        AVG(tip_amount) AS avg_tip\n",
        "    \"\"\", \"payment_type\")\n",
        "    .order(\"total_revenue DESC\")\n",
        "    .fetchdf()\n",
        ")\n",
        "\n",
        "print(payment_summary.head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAnfZZxhBdnt"
      },
      "source": [
        "### Extract Hour and Count Trips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7vfdfDTUCbR",
        "outputId": "e84ff4b4-d299-4893-adb2-98a56977e583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   pickup_hour  trip_count\n",
            "0            0       84969\n",
            "1            1       59799\n",
            "2            2       42040\n",
            "3            3       27438\n",
            "4            4       17835\n"
          ]
        }
      ],
      "source": [
        "hourly_trips = (\n",
        "    con.read_parquet(PARQUET_FILE)\n",
        "    .project(\"EXTRACT(HOUR FROM tpep_pickup_datetime) AS pickup_hour\")\n",
        "    .aggregate(\"pickup_hour, COUNT(*) AS trip_count\", \"pickup_hour\")\n",
        "    .order(\"pickup_hour\")\n",
        "    .fetchdf()\n",
        ")\n",
        "\n",
        "print(hourly_trips.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK4zJuPjBnvt"
      },
      "source": [
        "### Join with Taxi Zone Lookup (CSV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqQuNqBvUCbR",
        "outputId": "b007093a-e638-462e-bcdb-b9ff5228278b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   VendorID  trip_distance  total_amount    Borough\n",
            "0         2          11.70         64.44  Manhattan\n",
            "1         2          11.43         66.31     Queens\n",
            "2         1          17.80         95.15     Queens\n",
            "3         2          11.11         61.86     Queens\n",
            "4         2          16.02         64.85     Queens\n",
            "5         1          11.30         64.20  Manhattan\n",
            "6         2          11.19         72.90  Manhattan\n",
            "7         2          11.03         68.50     Queens\n",
            "8         2          13.54         78.41     Queens\n",
            "9         1          19.20         91.25     Queens\n"
          ]
        }
      ],
      "source": [
        "# Read both files as relations\n",
        "taxi_rel = con.read_parquet(PARQUET_FILE)\n",
        "zone_rel = con.read_csv(ZONE_FILE)\n",
        "\n",
        "# Perform join to get borough names\n",
        "joined = (\n",
        "    taxi_rel\n",
        "    .filter(\"trip_distance > 10\")\n",
        "    .join(\n",
        "        zone_rel,\n",
        "        condition=\"PULocationID = LocationID\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "    .project(\"VendorID, trip_distance, total_amount, Borough\")\n",
        "    .limit(10)\n",
        "    .fetchdf()\n",
        ")\n",
        "\n",
        "print(joined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ByMgKyLBp69"
      },
      "source": [
        "### Borough-Level Aggregation (After Join)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTY5ggU8UCbS",
        "outputId": "e0bfd773-431b-444f-ccb2-f6711640d7c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  pickup_borough  avg_distance  total_revenue\n",
            "0      Manhattan      2.883807   6.107196e+07\n",
            "1         Queens     12.320490   1.928394e+07\n",
            "2        Unknown      7.362869   1.704629e+06\n",
            "3       Brooklyn      5.677405   5.968103e+05\n",
            "4          Bronx      5.296593   1.437350e+05\n",
            "5            EWR      1.594780   4.279404e+04\n",
            "6  Staten Island     11.356774   2.132153e+04\n"
          ]
        }
      ],
      "source": [
        "borough_revenue = (\n",
        "    taxi_rel\n",
        "    .join(zone_rel, condition=\"PULocationID = LocationID\", how=\"left\")\n",
        "    .aggregate(\"\"\"\n",
        "        Borough AS pickup_borough,\n",
        "        AVG(trip_distance) AS avg_distance,\n",
        "        SUM(total_amount) AS total_revenue\n",
        "    \"\"\", \"Borough\")\n",
        "    .order(\"total_revenue DESC\")\n",
        "    .fetchdf()\n",
        ")\n",
        "\n",
        "print(borough_revenue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQLR5qUTB9dS"
      },
      "source": [
        "### Window Function (Top 3 Longest Trips per Payment Type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPK1cvIIUCbS",
        "outputId": "c5cffc25-1eeb-4ece-fc60-b1b5b4d0711c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    payment_type  trip_distance  rank\n",
            "0              0      258928.15     1\n",
            "1              0      225987.37     2\n",
            "2              0      187872.33     3\n",
            "3              1       14098.55     1\n",
            "4              1        9947.03     2\n",
            "5              1        9684.00     3\n",
            "6              2       62359.52     1\n",
            "7              2        9674.67     2\n",
            "8              2        6508.23     3\n",
            "9              3          74.40     1\n",
            "10             3          70.20     2\n",
            "11             3          63.60     3\n",
            "12             4          78.78     1\n",
            "13             4          78.78     2\n",
            "14             4          71.63     3\n"
          ]
        }
      ],
      "source": [
        "top_trips = (\n",
        "    con.read_parquet(PARQUET_FILE)\n",
        "    .project(\"payment_type, trip_distance\")\n",
        "    .filter(\"trip_distance IS NOT NULL AND trip_distance > 0\")\n",
        "    .project(\"\"\"\n",
        "        payment_type,\n",
        "        trip_distance,\n",
        "        ROW_NUMBER() OVER (\n",
        "            PARTITION BY payment_type\n",
        "            ORDER BY trip_distance DESC\n",
        "        ) AS rank\n",
        "    \"\"\")\n",
        "    .filter(\"rank <= 3\")\n",
        "    .order(\"payment_type, rank\")\n",
        "    .fetchdf()\n",
        ")\n",
        "\n",
        "print(top_trips)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxJCcJOACGLy"
      },
      "source": [
        "### Export Result to CSV (via Relation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJqqUszwUCbS",
        "outputId": "172c87dd-17b2-4830-bce8-fd0d7c52296a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Exported 1000 long trips to CSV\n"
          ]
        }
      ],
      "source": [
        "# Define a filtered relation\n",
        "long_trips = (\n",
        "    con.read_parquet(PARQUET_FILE)\n",
        "    .filter(\"trip_distance > 20\")\n",
        "    .project(\"VendorID, tpep_pickup_datetime, trip_distance, total_amount\")\n",
        "    .limit(1000)\n",
        ")\n",
        "\n",
        "# Write to CSV\n",
        "long_trips.write_csv(DATA_FOLDER + \"long_trips_output.csv\")\n",
        "print(\"✅ Exported 1000 long trips to CSV\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHomG3aKCKi_"
      },
      "source": [
        "### Mix Pandas DataFrame + DuckDB Relation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xE2VqwpjUCbT",
        "outputId": "7a346eba-722e-4129-8ea6-45735e075944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   avg_tip_in_selected_zones\n",
            "0                   6.467511\n"
          ]
        }
      ],
      "source": [
        "# Suppose you have a small Pandas filter list\n",
        "high_tip_zones = pd.DataFrame({\n",
        "    'LocationID': [1, 2, 132, 234]\n",
        "})\n",
        "\n",
        "# Register it in DuckDB\n",
        "con.register('high_tip_zones', high_tip_zones)\n",
        "\n",
        "# Join Parquet data with Pandas DataFrame\n",
        "result = (\n",
        "    con.read_parquet(PARQUET_FILE)\n",
        "    .join(con.table('high_tip_zones'), 'PULocationID = LocationID')\n",
        "    .aggregate(\"AVG(tip_amount) AS avg_tip_in_selected_zones\")\n",
        "    .fetchdf()\n",
        ")\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsXxXJjXUCbT"
      },
      "outputs": [],
      "source": [
        "con.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv3NodgPPM8s"
      },
      "source": [
        "## Machine Learning Preprocessing with DuckDB\n",
        "\n",
        "DuckDB is not a machine learning library, but it plays a powerful supporting role in ML workflows—especially in the data preprocessing, feature engineering, and scalable inference stages. By performing these steps inside the database, you avoid costly data movement, reduce memory pressure, and often achieve significant performance gains over traditional Python-based pipelines (e.g., scikit-learn + Pandas).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvkYZMkfDX9Q"
      },
      "source": [
        " ### Why Use DuckDB for ML?\n",
        "\n",
        "| Benefit | Explanation |\n",
        "|--------|-------------|\n",
        "| **Speed** | Vectorized, parallel SQL execution often outperforms Pandas/scikit-learn on large data |\n",
        "| **Memory Efficiency** | Processes data in chunks; no need to load full dataset into RAM |\n",
        "| **Reproducibility** | Preprocessing logic lives in SQL—easy to version, audit, and reuse |\n",
        "| **Data Locality** | Transform data where it lives (Parquet, CSV, S3, etc.) |\n",
        "| **Consistency** | Same SQL logic can be used at training **and** inference time |\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvpZNMaRD1UB"
      },
      "source": [
        "Below is a practical guide to using **DuckDB for ML preprocessing and inference**, based on real-world patterns from the [DuckDB ML preprocessing blog](https://duckdb.org/2025/08/15/ml-data-preprocessing.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AW554_EEYe4"
      },
      "source": [
        "### Load Data\n",
        "We’ll use the synthetic financial fraud dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cpSRBWjUCbU",
        "outputId": "527664e0-290a-4f35-8ea2-e8369d1c485e",
        "colab": {
          "referenced_widgets": [
            "9185787ffa4c4fa28265c28fd8622650"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9185787ffa4c4fa28265c28fd8622650",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import duckdb\n",
        "\n",
        "# Load data directly from URL into a table\n",
        "duckdb.sql(\"\"\"\n",
        "    CREATE TABLE financial_trx AS\n",
        "    FROM read_csv('https://blobs.duckdb.org/data/financial_fraud_detection_dataset.csv')\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "che6NCa3EjXf"
      },
      "source": [
        "### Explore & Summarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16t7AKpHUCbU",
        "outputId": "58acb3a9-c15e-4668-ddfc-027a5107b6b6",
        "colab": {
          "referenced_widgets": [
            "0c72675c7f6b4bb4adebee524195f2e5"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c72675c7f6b4bb4adebee524195f2e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    column_name column_type  null_percentage  \\\n",
            "0                transaction_id     VARCHAR             0.00   \n",
            "1                     timestamp   TIMESTAMP             0.00   \n",
            "2                sender_account     VARCHAR             0.00   \n",
            "3              receiver_account     VARCHAR             0.00   \n",
            "4                        amount      DOUBLE             0.00   \n",
            "5              transaction_type     VARCHAR             0.00   \n",
            "6             merchant_category     VARCHAR             0.00   \n",
            "7                      location     VARCHAR             0.00   \n",
            "8                   device_used     VARCHAR             0.00   \n",
            "9                      is_fraud     BOOLEAN             0.00   \n",
            "10                   fraud_type     VARCHAR            96.41   \n",
            "11  time_since_last_transaction      DOUBLE            17.93   \n",
            "12     spending_deviation_score      DOUBLE             0.00   \n",
            "13               velocity_score      BIGINT             0.00   \n",
            "14            geo_anomaly_score      DOUBLE             0.00   \n",
            "15              payment_channel     VARCHAR             0.00   \n",
            "16                   ip_address     VARCHAR             0.00   \n",
            "17                  device_hash     VARCHAR             0.00   \n",
            "\n",
            "                           min  \n",
            "0                      T100000  \n",
            "1   2023-01-01 00:09:26.241974  \n",
            "2                    ACC100000  \n",
            "3                    ACC100000  \n",
            "4                         0.01  \n",
            "5                      deposit  \n",
            "6                entertainment  \n",
            "7                       Berlin  \n",
            "8                          atm  \n",
            "9                        false  \n",
            "10            card_not_present  \n",
            "11          -8777.814181944444  \n",
            "12                       -5.26  \n",
            "13                           1  \n",
            "14                         0.0  \n",
            "15                         ACH  \n",
            "16                 0.0.102.150  \n",
            "17                    D1000002  \n"
          ]
        }
      ],
      "source": [
        "# Get column stats (null %, min, type, etc.)\n",
        "summary = duckdb.sql(\"\"\"\n",
        "    FROM (SUMMARIZE financial_trx)\n",
        "    SELECT column_name, column_type, null_percentage, min\n",
        "\"\"\").fetchdf()\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNQaynW5EldN"
      },
      "source": [
        "### Feature Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92nCgSObUCbU"
      },
      "outputs": [],
      "source": [
        "one_hot = duckdb.sql(\"\"\"\n",
        "    SELECT DISTINCT\n",
        "        transaction_type,\n",
        "        (transaction_type = 'deposit')::INT AS deposit_onehot,\n",
        "        (transaction_type = 'payment')::INT AS payment_onehot,\n",
        "        (transaction_type = 'transfer')::INT AS transfer_onehot,\n",
        "        (transaction_type = 'withdrawal')::INT AS withdrawal_onehot\n",
        "    FROM financial_trx\n",
        "    ORDER BY transaction_type\n",
        "\"\"\").fetchdf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWIigg6FE1Sa"
      },
      "source": [
        "### Ordinal Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O45OHyYQUCbV"
      },
      "outputs": [],
      "source": [
        "ordinal = duckdb.sql(\"\"\"\n",
        "    WITH trx_encoded AS (\n",
        "        SELECT\n",
        "            transaction_type,\n",
        "            ROW_NUMBER() OVER (ORDER BY transaction_type) - 1 AS trx_oe\n",
        "        FROM (SELECT DISTINCT transaction_type FROM financial_trx)\n",
        "    )\n",
        "    SELECT f.transaction_type, e.trx_oe, COUNT(*) AS count\n",
        "    FROM financial_trx f\n",
        "    JOIN trx_encoded e ON f.transaction_type = e.transaction_type\n",
        "    GROUP BY ALL\n",
        "    ORDER BY trx_oe\n",
        "\"\"\").fetchdf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB64VWb-E6kJ"
      },
      "source": [
        "### Train/Test Split (Avoid Data Leakage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4kdnKNjUCbV",
        "outputId": "dee5c37c-200d-4ec4-fce9-1c388461b816",
        "colab": {
          "referenced_widgets": [
            "2e4f4dc615bd4469ba9c05cca04a9e11",
            "17957481abff4d3db622740eae1d076b"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e4f4dc615bd4469ba9c05cca04a9e11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17957481abff4d3db622740eae1d076b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Reproducible 80/20 split\n",
        "duckdb.sql(\"SET threads = 1\")  # for reproducibility\n",
        "\n",
        "duckdb.sql(\"\"\"\n",
        "    CREATE TABLE financial_trx_training AS\n",
        "    FROM financial_trx\n",
        "    USING SAMPLE 80 PERCENT (reservoir, 256)\n",
        "\"\"\")\n",
        "\n",
        "duckdb.sql(\"\"\"\n",
        "    CREATE TABLE financial_trx_testing AS\n",
        "    FROM financial_trx\n",
        "    ANTI JOIN financial_trx_training USING (transaction_id)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EMZSj58FCEI"
      },
      "source": [
        "### Feature Scaling (Using Macros)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-xkHefxUCbV"
      },
      "outputs": [],
      "source": [
        "# Standard scaler: (x - mean) / std\n",
        "duckdb.sql(\"\"\"\n",
        "    CREATE OR REPLACE MACRO standard_scaler(x, mean_x, std_x) AS\n",
        "        (x - mean_x) / NULLIF(std_x, 0)\n",
        "\"\"\")\n",
        "\n",
        "# Min-Max scaler: (x - min) / (max - min)\n",
        "duckdb.sql(\"\"\"\n",
        "    CREATE OR REPLACE MACRO min_max_scaler(x, min_x, max_x) AS\n",
        "        (x - min_x) / NULLIF(max_x - min_x, 0)\n",
        "\"\"\")\n",
        "\n",
        "# Robust scaler: (x - median) / IQR\n",
        "duckdb.sql(\"\"\"\n",
        "    CREATE OR REPLACE MACRO robust_scaler(x, q25, q50, q75) AS\n",
        "        (x - q50) / NULLIF(q75 - q25, 0)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxb-nHm3FJ2c"
      },
      "source": [
        "### Compute Scaling Parameters from Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeYGaPHbUCbV"
      },
      "outputs": [],
      "source": [
        "params = duckdb.sql(\"\"\"\n",
        "    FROM financial_trx_training\n",
        "    SELECT\n",
        "        AVG(velocity_score) AS avg_v,\n",
        "        STDDEV_POP(velocity_score) AS std_v,\n",
        "        MIN(velocity_score) AS min_v,\n",
        "        MAX(velocity_score) AS max_v,\n",
        "        QUANTILE_CONT(velocity_score, 0.25) AS q25_v,\n",
        "        MEDIAN(velocity_score) AS med_v,\n",
        "        QUANTILE_CONT(velocity_score, 0.75) AS q75_v\n",
        "\"\"\").fetchdf().iloc[0].to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7BFA7dZFPjx"
      },
      "source": [
        "### Apply Scaling on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTu-1IcIUCbW",
        "outputId": "29cdefbc-b11f-4d66-ae98-3431d9389155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       ss_v      mm_v  rs_v\n",
            "0 -1.300957  0.105263  -0.8\n",
            "1  0.433105  0.631579   0.2\n",
            "2 -0.607332  0.315789  -0.4\n",
            "3 -0.954145  0.210526  -0.6\n",
            "4  0.086293  0.526316   0.0\n"
          ]
        }
      ],
      "source": [
        "scaled_test = duckdb.sql(f\"\"\"\n",
        "    SELECT\n",
        "        standard_scaler(velocity_score, {params['avg_v']}, {params['std_v']}) AS ss_v,\n",
        "        min_max_scaler(velocity_score, {params['min_v']}, {params['max_v']}) AS mm_v,\n",
        "        robust_scaler(velocity_score, {params['q25_v']}, {params['med_v']}, {params['q75_v']}) AS rs_v\n",
        "    FROM financial_trx_testing\n",
        "    LIMIT 5\n",
        "\"\"\").fetchdf()\n",
        "print(scaled_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5wNyiqOUCbW",
        "outputId": "2bbf8e77-ef0b-46dc-dd71-84063192dc1e",
        "colab": {
          "referenced_widgets": [
            "cb15e717652b4b13817bde1698d0ab99",
            "88e069a674fd40499cc756a56d4db978"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb15e717652b4b13817bde1698d0ab99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88e069a674fd40499cc756a56d4db978",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        column_name column_type    count  null_percentage  \\\n",
            "0    transaction_id     VARCHAR  5000000              0.0   \n",
            "1         timestamp   TIMESTAMP  5000000              0.0   \n",
            "2    sender_account     VARCHAR  5000000              0.0   \n",
            "3  receiver_account     VARCHAR  5000000              0.0   \n",
            "4            amount      DOUBLE  5000000              0.0   \n",
            "\n",
            "                          min  \n",
            "0                     T100000  \n",
            "1  2023-01-01 00:09:26.241974  \n",
            "2                   ACC100000  \n",
            "3                   ACC100000  \n",
            "4                        0.01  \n",
            "  transaction_type  deposit_onehot  payment_onehot\n",
            "0          deposit               1               0\n",
            "1          payment               0               1\n",
            "2         transfer               0               0\n",
            "3       withdrawal               0               0\n"
          ]
        }
      ],
      "source": [
        "# Load financial fraud dataset\n",
        "con = duckdb.connect()\n",
        "con.execute(\"\"\"\n",
        "CREATE TABLE financial_trx AS\n",
        "FROM read_csv('https://blobs.duckdb.org/data/financial_fraud_detection_dataset.csv')\n",
        "\"\"\")\n",
        "\n",
        "# Summarize\n",
        "summary = con.execute(\"\"\"\n",
        "FROM (SUMMARIZE financial_trx)\n",
        "SELECT column_name, column_type, count, null_percentage, min\n",
        "\"\"\").fetchdf()\n",
        "print(summary.head())\n",
        "\n",
        "# One-hot encoding example\n",
        "one_hot = con.execute(\"\"\"\n",
        "SELECT DISTINCT\n",
        "    transaction_type,\n",
        "    (transaction_type = 'deposit')::INT AS deposit_onehot,\n",
        "    (transaction_type = 'payment')::INT AS payment_onehot\n",
        "FROM financial_trx\n",
        "ORDER BY transaction_type\n",
        "\"\"\").fetchdf()\n",
        "print(one_hot)\n",
        "\n",
        "con.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMB8HmcPFV5v"
      },
      "source": [
        "### Handle Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvQ_LXElUCbW",
        "outputId": "bf03f7c4-02cc-437f-e635-37801bb2775c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   fill_0  fill_mean\n",
            "0     0.0    1.90919\n",
            "1     0.0    1.90919\n",
            "2     0.0    1.90919\n"
          ]
        }
      ],
      "source": [
        "# Get mean from training set\n",
        "mean_time = duckdb.sql(\"\"\"\n",
        "    SELECT AVG(time_since_last_transaction)\n",
        "    FROM financial_trx_training\n",
        "\"\"\").fetchone()[0]\n",
        "\n",
        "# Impute in test set\n",
        "imputed = duckdb.sql(f\"\"\"\n",
        "    SELECT\n",
        "        COALESCE(time_since_last_transaction, 0) AS fill_0,\n",
        "        COALESCE(time_since_last_transaction, {mean_time}) AS fill_mean\n",
        "    FROM financial_trx_testing\n",
        "    WHERE time_since_last_transaction IS NULL\n",
        "    LIMIT 3\n",
        "\"\"\").fetchdf()\n",
        "print(imputed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCltb4AZFbmv"
      },
      "source": [
        "### Export Features for Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAYouzfCUCbX",
        "outputId": "0b5f9052-e80a-44e4-e836-008443ed1441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training features saved to train_features.parquet\n"
          ]
        }
      ],
      "source": [
        "# Final preprocessed training set\n",
        "duckdb.sql(\"\"\"\n",
        "    COPY (\n",
        "        SELECT\n",
        "            transaction_id,\n",
        "            -- Encoded features\n",
        "            (transaction_type = 'deposit')::INT AS is_deposit,\n",
        "            -- Scaled features\n",
        "            (velocity_score - (SELECT AVG(velocity_score) FROM financial_trx_training)) /\n",
        "            (SELECT STDDEV_POP(velocity_score) FROM financial_trx_training) AS ss_velocity,\n",
        "            -- Imputed features\n",
        "            COALESCE(time_since_last_transaction,\n",
        "                     (SELECT AVG(time_since_last_transaction) FROM financial_trx_training)) AS time_filled,\n",
        "            -- Target\n",
        "            is_fraud\n",
        "        FROM financial_trx_training\n",
        "    ) TO 'train_features.parquet' (FORMAT PARQUET)\n",
        "\"\"\")\n",
        "\n",
        "print( \"Training features saved to train_features.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZTy6DScFs5p"
      },
      "source": [
        "### Consistent Inference\n",
        "\n",
        "At inference time, reuse the same SQL logic with precomputed parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwDcH34NUCbX",
        "outputId": "f83fa9cd-a266-4dfb-a60a-898c34787796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  transaction_id  ss_velocity  time_filled\n",
            "0        T_new_1     0.779918     10.50000\n",
            "1        T_new_2    -1.300957      1.90919\n",
            "2        T_new_3     1.300136      5.20000\n"
          ]
        }
      ],
      "source": [
        "# Example: new transaction\n",
        "# Create a temporary table with sample new transaction data for demonstration\n",
        "con = duckdb.connect() # Use a new in-memory connection for this example\n",
        "con.execute(\"\"\"\n",
        "    CREATE TEMP TABLE new_transactions (\n",
        "        transaction_id VARCHAR,\n",
        "        velocity_score DOUBLE,\n",
        "        time_since_last_transaction DOUBLE\n",
        "    );\n",
        "    INSERT INTO new_transactions VALUES\n",
        "    ('T_new_1', 15.0, 10.5),\n",
        "    ('T_new_2', 3.0, NULL), -- Simulate missing value\n",
        "    ('T_new_3', 18.0, 5.2);\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "new_data = con.sql(f\"\"\"\n",
        "    SELECT\n",
        "        transaction_id,\n",
        "        (velocity_score - {params['avg_v']}) / {params['std_v']} AS ss_velocity,  -- precomputed mean/std from training\n",
        "        COALESCE(time_since_last_transaction, {mean_time}) AS time_filled  -- precomputed mean from training\n",
        "    FROM new_transactions\n",
        "\"\"\").fetchdf()\n",
        "\n",
        "print(new_data)\n",
        "\n",
        "con.close() # Close the temporary connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBkIskYF7E-"
      },
      "source": [
        "## Benchmark Test: DuckDB vs scikit-learn\n",
        "\n",
        "According to DuckDB’s official benchmark:\n",
        "\n",
        "* `DuckDB is 2–10x faste`r than scikit-learn pipelines for preprocessing\n",
        "* `Lower memory usage` (streaming vs full materialization)\n",
        "* `Same numerical result`s (validated via reconciliation scripts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR0p6s3tICwo"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff5soz4FUCbX",
        "outputId": "e26b38a7-2fe1-4172-8464-98591d813201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data into Pandas...\n",
            "Loaded 5,000,000 rows in 47.65s\n"
          ]
        }
      ],
      "source": [
        "import duckdb\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Load data into Pandas (for scikit-learn)\n",
        "print(\"Loading data into Pandas...\")\n",
        "start = time.time()\n",
        "df = pd.read_csv('https://blobs.duckdb.org/data/financial_fraud_detection_dataset.csv')\n",
        "load_time = time.time() - start\n",
        "print(f\"Loaded {len(df):,} rows in {load_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTnxqRILIJdX"
      },
      "source": [
        "### scikit-learn Pipeline\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUUxD-XUUCbY"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def sklearn_preprocess(df):\n",
        "    # Train/test split\n",
        "    train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Categorical & numerical columns\n",
        "    cat_cols = ['transaction_type', 'payment_channel']\n",
        "    num_cols = ['velocity_score', 'spending_deviation_score']\n",
        "    miss_col = ['time_since_last_transaction']\n",
        "\n",
        "    # Preprocessor\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('onehot', OneHotEncoder(drop='first', sparse_output=False), cat_cols),\n",
        "        ('standard', StandardScaler(), ['velocity_score']),\n",
        "        ('minmax', MinMaxScaler(), ['spending_deviation_score']),\n",
        "        ('impute_mean', SimpleImputer(strategy='mean'), miss_col),\n",
        "        ('robust', RobustScaler(), ['amount'])  # extra robust scaling\n",
        "    ], remainder='drop')\n",
        "\n",
        "    # Fit on train, transform test\n",
        "    preprocessor.fit(train)\n",
        "    X_test_transformed = preprocessor.transform(test)\n",
        "    return X_test_transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHIi6SWhIOz1"
      },
      "source": [
        "### DuckDB Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcqECEBDUCbY"
      },
      "outputs": [],
      "source": [
        "def duckdb_preprocess():\n",
        "    # Use DuckDB to do everything in SQL (no Pandas load needed!)\n",
        "    con = duckdb.connect()\n",
        "\n",
        "    # Create train/test split\n",
        "    con.execute(\"SET threads = 1\")  # reproducibility\n",
        "    con.execute(\"\"\"\n",
        "        CREATE TABLE train AS\n",
        "        FROM read_csv('https://blobs.duckdb.org/data/financial_fraud_detection_dataset.csv')\n",
        "        USING SAMPLE 80 PERCENT (reservoir, 256)\n",
        "    \"\"\")\n",
        "    con.execute(\"\"\"\n",
        "        CREATE TABLE test AS\n",
        "        FROM read_csv('https://blobs.duckdb.org/data/financial_fraud_detection_dataset.csv') t\n",
        "        ANTI JOIN train USING (transaction_id)\n",
        "    \"\"\")\n",
        "    con.execute(\"SET threads = 8\")\n",
        "\n",
        "    # Compute scaling/imputation params from TRAIN\n",
        "    params = con.execute(\"\"\"\n",
        "        SELECT\n",
        "            AVG(velocity_score) AS avg_v,\n",
        "            STDDEV_POP(velocity_score) AS std_v,\n",
        "            MIN(spending_deviation_score) AS min_s,\n",
        "            MAX(spending_deviation_score) AS max_s,\n",
        "            AVG(time_since_last_transaction) AS mean_t,\n",
        "            MEDIAN(amount) AS med_a,\n",
        "            QUANTILE_CONT(amount, 0.25) AS q25_a,\n",
        "            QUANTILE_CONT(amount, 0.75) AS q75_a\n",
        "        FROM train\n",
        "    \"\"\").fetchdf().iloc[0].to_dict()\n",
        "\n",
        "    # Apply full preprocessing on TEST\n",
        "    result = con.execute(f\"\"\"\n",
        "        SELECT\n",
        "            -- One-hot (drop first: deposit = baseline for transaction_type, ACH for payment_channel)\n",
        "            (transaction_type = 'payment')::DOUBLE AS transaction_type_payment,\n",
        "            (transaction_type = 'transfer')::DOUBLE AS transaction_type_transfer,\n",
        "            (transaction_type = 'withdrawal')::DOUBLE AS transaction_type_withdrawal,\n",
        "            (payment_channel = 'card')::DOUBLE AS payment_channel_card,\n",
        "            (payment_channel = 'credit_card')::DOUBLE AS payment_channel_credit_card,\n",
        "            (payment_channel = 'debit_card')::DOUBLE AS payment_channel_debit_card,\n",
        "            (payment_channel = 'wire_transfer')::DOUBLE AS payment_channel_wire_transfer,\n",
        "            -- Scaling\n",
        "            (velocity_score - {params['avg_v']}) / {params['std_v']} AS velocity_score_standard,\n",
        "            (spending_deviation_score - {params['min_s']}) / NULLIF({params['max_s']} - {params['min_s']}, 0) AS spending_deviation_score_minmax,\n",
        "            COALESCE(time_since_last_transaction, {params['mean_t']}) AS time_since_last_transaction_impute_mean,\n",
        "            -- Robust scaling for amount\n",
        "            (amount - {params['med_a']}) / NULLIF({params['q75_a']} - {params['q25_a']}, 0) AS amount_robust\n",
        "        FROM test\n",
        "    \"\"\").fetchdf()\n",
        "\n",
        "    con.close()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_4_5K1SH1g9"
      },
      "source": [
        "### Run Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oXF1s_jUCbY",
        "outputId": "db3d7976-8c5a-4e77-b4bc-e7327a92ccb1",
        "colab": {
          "referenced_widgets": [
            "4419e703136e404c8481a6dd24bf52cc",
            "d0fea3ed7d064f77b02ce7c490a0599f"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Running scikit-learn pipeline...\n",
            " scikit-learn finished in 23.13s\n",
            "   Output shape: (1000000, 10)\n",
            "\n",
            " Running DuckDB pipeline...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4419e703136e404c8481a6dd24bf52cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0fea3ed7d064f77b02ce7c490a0599f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " DuckDB finished in 44.05s\n",
            "   Output shape: (1000000, 11)\n"
          ]
        }
      ],
      "source": [
        "# --- scikit-learn ---\n",
        "print(\"\\n Running scikit-learn pipeline...\")\n",
        "start = time.time()\n",
        "X_sklearn = sklearn_preprocess(df)\n",
        "sklearn_time = time.time() - start\n",
        "print(f\" scikit-learn finished in {sklearn_time:.2f}s\")\n",
        "print(f\"   Output shape: {X_sklearn.shape}\")\n",
        "\n",
        "# --- DuckDB ---\n",
        "print(\"\\n Running DuckDB pipeline...\")\n",
        "start = time.time()\n",
        "df_duckdb = duckdb_preprocess()\n",
        "duckdb_time = time.time() - start\n",
        "print(f\" DuckDB finished in {duckdb_time:.2f}s\")\n",
        "print(f\"   Output shape: {df_duckdb.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "866274e7"
      },
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "This tutorial demonstrated the power and flexibility of using DuckDB in Python for efficient data analysis and preprocessing, especially with large datasets. We covered:\n",
        "\n",
        "- **Core Concepts**: DuckDB's in-process nature, columnar storage, and SQL capabilities.\n",
        "- **Integration**: Seamless integration with popular Python libraries like Pandas, Polars, and Ibis.\n",
        "- **Data Handling**: Directly querying various file formats (CSV, Parquet) without loading them into memory, handling large datasets efficiently.\n",
        "- **SQL Execution**: Executing SQL queries directly or through connections, and converting results to different formats.\n",
        "- **Relational API**: Using the relational API for lazy evaluation and optimized query execution.\n",
        "- **Machine Learning Preprocessing**: Applying DuckDB for feature encoding, scaling, imputation, and train/test splitting directly within the database, highlighting its performance benefits over traditional methods like scikit-learn for large datasets.\n",
        "\n",
        "In conclusion, DuckDB is a valuable tool for data professionals working with large datasets in Python, offering significant advantages in terms of speed, memory efficiency, and ease of use through its SQL interface and integrations. Its ability to process data directly from files and perform complex transformations within the database makes it particularly well-suited for modern data workflows, including those involving machine learning preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6da90ee1"
      },
      "source": [
        "## Resources\n",
        "\n",
        "1.  [DuckDB Documentation](https://duckdb.org/docs/)\n",
        "2.  [DuckDB Python API Reference](https://duckdb.org/docs/api/python/overview)\n",
        "3.  [DuckDB Blog](https://duckdb.org/blog/)\n",
        "4.  [DuckDB GitHub Repository](https://github.com/duckdb/duckdb)\n",
        "5.  [Ibis Project Documentation (for Ibis + DuckDB)](https://ibis-project.org/backends/duckdb)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}