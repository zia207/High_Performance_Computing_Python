{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://drive.google.com/uc?export=view&id=1IFEWet-Aw4DhkkVe1xv_2YYqlvRe9m5_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data with High-Performance Computing (HPC) in Python\n",
    "\n",
    "Handling big data with high-performance computing (HPC) in Python involves leveraging specialized libraries, distributed frameworks, and hardware acceleration to process massive datasets efficiently. Unlike R — which traditionally operates in-memory — Python’s ecosystem is natively designed for scalability, from single-machine parallelism to distributed clusters and GPU computing. Below is a comprehensive guide to implementing big data solutions in Python using HPC techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Big Data** refers to datasets too large or complex for traditional tools to handle, characterized by the “5 Vs”:\n",
    "\n",
    "- **Volume**: Scale of data — terabytes to petabytes.\n",
    "- **Velocity**: Speed of data ingestion and processing — real-time streams, batch updates.\n",
    "- **Variety**: Heterogeneous formats — structured (SQL tables), semi-structured (JSON, XML), unstructured (text, images).\n",
    "- **Veracity**: Data quality, noise, and uncertainty.\n",
    "- **Value**: Actionable insights derived through analytics, ML, or visualization.\n",
    "\n",
    "Python’s rich ecosystem — including NumPy, Pandas, Dask, PySpark, CuPy, and Ray — enables scalable, performant big data workflows across single machines, multi-core systems, GPU clusters, and cloud environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Approaches for Big Data with HPC in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Single-Node Parallelism\n",
    "\n",
    "Leverage multiple CPU cores on a single machine for embarrassingly parallel tasks (e.g., Monte Carlo simulations, hyperparameter tuning).\n",
    "\n",
    "- **multiprocessing**: Built-in Python library for spawning processes.\n",
    "- **concurrent.futures**: High-level interface for asynchronously executing callables.\n",
    "- **joblib**: Optimized for scientific computing; integrates with scikit-learn.\n",
    "- **Numba**: JIT compiler for speeding up NumPy-heavy code with `@jit` decorators.\n",
    "- **Cython**: Write C extensions for performance-critical loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "def compute(x):\n",
    "    return np.sqrt(x**2 + 1)\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(compute)(i) for i in range(1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-Node & Distributed Computing\n",
    "\n",
    "Scale computations across clusters using frameworks like Dask, PySpark, or Ray.\n",
    "\n",
    "- **Dask**: Native Python parallel computing library. Integrates with Pandas, NumPy, Scikit-learn. Supports task scheduling and lazy evaluation.\n",
    "- **PySpark**: Python API for Apache Spark. Ideal for ETL, SQL, streaming, and ML at scale.\n",
    "- **Ray**: General-purpose distributed computing framework. Powers libraries like Modin, Tune, and RLlib.\n",
    "- **MPI for Python (mpi4py)**: Message Passing Interface for HPC clusters — used in scientific computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv('large_dataset*.csv')\n",
    "result = df.groupby('category').value.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Memory-Efficient & Out-of-Core Processing\n",
    "\n",
    "Process datasets larger than RAM using lazy evaluation, chunking, or disk-backed arrays.\n",
    "\n",
    "- **Dask**: Breaks datasets into partitions; computes lazily.\n",
    "- **Vaex**: Virtual DataFrames for out-of-core analytics — zero memory copy, memory-mapped.\n",
    "- **Polars**: Blazingly fast DataFrame library written in Rust; supports lazy evaluation and streaming.\n",
    "- **HDF5 / Zarr**: Efficient storage formats for large numerical arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaex\n",
    "df = vaex.open('huge_dataset.hdf5')\n",
    "df_filtered = df[df.salary > 100000]\n",
    "mean_age = df_filtered.age.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Database & Cloud Integration\n",
    "\n",
    "Offload computation to databases or cloud warehouses to avoid loading data locally.\n",
    "\n",
    "- **SQLAlchemy / DuckDB**: Query databases directly from Python. DuckDB is embeddable, columnar, and optimized for analytical workloads.\n",
    "- **BigQuery / Snowflake / Redshift connectors**: Use `pandas.read_gbq()` or SQLAlchemy to query cloud warehouses.\n",
    "- **fsspec + s3fs / gcsfs**: Read data directly from cloud storage (S3, GCS) without downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "con = duckdb.connect()\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT category, AVG(salary)\n",
    "    FROM 's3://bucket/large_data.parquet'\n",
    "    GROUP BY category\n",
    "\"\"\").fetchdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. GPU-Accelerated Computing\n",
    "\n",
    "Use GPUs for massive parallelization — especially for ML, array math, and graph analytics.\n",
    "\n",
    "- **CuPy**: NumPy-compatible array library for NVIDIA GPUs.\n",
    "- **RAPIDS (cuDF, cuML, cuGraph)**: GPU-accelerated data science libraries mirroring Pandas, Scikit-learn, NetworkX.\n",
    "- **PyTorch / TensorFlow**: Deep learning frameworks with GPU support and distributed training.\n",
    "- **Numba + CUDA**: Write custom GPU kernels in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "df = cudf.read_parquet('large_data.parquet')\n",
    "result = df.groupby('category').agg({'sales': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Optimized Storage Formats\n",
    "\n",
    "Use columnar, compressed, splittable formats for fast I/O and partial reads.\n",
    "\n",
    "- **Parquet**: Columnar storage, predicate pushdown, schema evolution.\n",
    "- **ORC**: Optimized Row Columnar — high compression, good for Hive/Spark.\n",
    "- **Feather**: Fast, language-agnostic binary format (via Arrow).\n",
    "- **Zstandard / Snappy**: Compression codecs for faster reads/writes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df.to_parquet('data.parquet', compression='snappy')\n",
    "df = pd.read_parquet('data.parquet', columns=['col1', 'col2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Strategies for Big Data Processing in Python\n",
    "\n",
    "These strategies can be combined based on data size, infrastructure, and latency requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Sample and Model**\n",
    "\n",
    "Downsample data for rapid prototyping, hyperparameter tuning, or feature engineering.\n",
    "\n",
    "- Fast iteration, low resource usage.\n",
    "- May miss rare events or long-tail patterns.\n",
    "- Tools: `DataFrame.sample()`, `sklearn.utils.resample`, `imbalanced-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = df.sample(n=100000, random_state=42)\n",
    "model.fit(sampled[features], sampled[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Chunk and Process**\n",
    "\n",
    "Split data into chunks (by time, category, or file) and process sequentially or in parallel.\n",
    "\n",
    "- Uses full dataset; embarrassingly parallel.\n",
    "- Requires chunkable data; I/O can bottleneck.\n",
    "- Tools: `pd.read_csv(chunksize=)`, `dask`, `multiprocessing.Pool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in pd.read_csv('big.csv', chunksize=10000):\n",
    "    process(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Push Compute to Storage**\n",
    "\n",
    "Filter, aggregate, or transform data where it lives — in databases, data lakes, or cloud storage.\n",
    "\n",
    "- Minimizes data movement; leverages optimized engines.\n",
    "- Limited by query capabilities of backend.\n",
    "- Tools: `DuckDB`, `SQLAlchemy`, `PySpark`, `dask-sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "ddf = dd.read_parquet('s3://bucket/data/*.parquet')\n",
    "result = ddf[ddf.year == 2023].groupby('region').sales.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. **Lazy Evaluation & Streaming**\n",
    "\n",
    "Defer computation until necessary; process data in streams without full materialization.\n",
    "\n",
    "- Memory efficient; pipelines scale well.\n",
    "- Debugging can be harder; not all ops supported.\n",
    "- Tools: `Dask`, `Polars (scan_parquet)`, `Vaex`, `PySpark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "lazy_df = pl.scan_parquet(\"data.parquet\")\n",
    "result = lazy_df.filter(pl.col(\"score\") > 0.5).group_by(\"user\").agg(pl.mean(\"score\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. **Distributed Computing**\n",
    "\n",
    "Scale horizontally across clusters for terabyte+ datasets or complex ML pipelines.\n",
    "\n",
    "- Massive scalability; fault tolerance.\n",
    "- Cluster setup complexity; serialization overhead.\n",
    "- Tools: `Dask.distributed`, `PySpark`, `Ray`, `mpi4py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client('scheduler-address:8786')  # Connect to cluster\n",
    "futures = client.map(process_file, file_list)\n",
    "results = client.gather(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. **GPU Acceleration**\n",
    "\n",
    "Offload compute-intensive tasks to GPUs for 10x–100x speedups.\n",
    "\n",
    "- Extreme performance for ML, math, graph ops.\n",
    "- Hardware dependency; memory constraints on GPU.\n",
    "- Tools: `RAPIDS`, `CuPy`, `PyTorch`, `TensorFlow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf, cuml\n",
    "df = cudf.read_csv('data.csv')\n",
    "kmeans = cuml.KMeans(n_clusters=5)\n",
    "clusters = kmeans.fit_predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Key Python Tools for Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Tool               | Best For                          | Scalability         | Parallelism        | Memory Model       | Learning Curve     |\n",
    "|--------------------|-----------------------------------|---------------------|--------------------|--------------------|--------------------|\n",
    "| **Pandas**         | Small-to-medium data (<10 GB)     | Single machine      | None (GIL-bound)   | In-memory          | Low                |\n",
    "| **Dask**           | Medium-to-large data, scaling Pandas/NumPy | Single to cluster   | Task-based         | Lazy / chunked     | Medium             |\n",
    "| **Polars**         | Fast DataFrame ops, streaming     | Single machine      | Multi-threaded     | In-memory / lazy   | Low-Medium         |\n",
    "| **Vaex**           | Out-of-core DataFrames            | Single machine      | Multi-threaded     | Memory-mapped      | Medium             |\n",
    "| **PySpark**        | Enterprise-scale ETL, SQL, ML     | Cluster (1000s nodes) | Distributed        | In-memory / disk   | High               |\n",
    "| **RAPIDS (cuDF)**  | GPU-accelerated DataFrames        | Single to multi-GPU | GPU-parallel       | GPU memory         | Medium             |\n",
    "| **Ray**            | General distributed computing     | Cluster             | Actor/task model   | Distributed        | Medium-High        |\n",
    "| **DuckDB**         | Analytical queries on local files | Single machine      | Multi-threaded     | In-memory / disk   | Low                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- **Memory Management**: Even with lazy evaluation, accidental materialization can crash processes.\n",
    "- **Serialization Overhead**: Distributed frameworks (Spark, Dask, Ray) require data serialization — can bottleneck performance.\n",
    "- **GPU Memory Limits**: GPU VRAM is often smaller than system RAM — requires chunking or memory pooling.\n",
    "- **Cluster Complexity**: Setting up and tuning distributed clusters (YARN, Kubernetes, Slurm) requires DevOps skills.\n",
    "- **I/O Bottlenecks**: Reading from disk or cloud storage can dominate runtime — use columnar formats and predicate pushdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "- **Start Small**: Prototype with Pandas or Polars on samples before scaling.\n",
    "- **Choose the Right Tool**:\n",
    "  - <10 GB → Pandas/Polars.\n",
    "  - 10 GB–1 TB → Dask/Vaex/DuckDB.\n",
    "  - >1 TB → PySpark/Ray.\n",
    "  - GPU available → RAPIDS/CuPy.\n",
    "- **Optimize I/O**: Use Parquet/ORC with compression; read only needed columns.\n",
    "- **Leverage Cloud**: Use AWS EMR, Databricks, or Google Dataproc for managed Spark clusters.\n",
    "- **Profile & Monitor**: Use `cProfile`, `snakeviz`, `dask.diagnostics`, or `nsys` (for GPU) to find bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "Python’s ecosystem for big data and HPC is mature, flexible, and performant — spanning from single-machine optimizations (Numba, Polars) to distributed clusters (Dask, Spark, Ray) and GPU acceleration (RAPIDS, CuPy). By combining sampling, chunking, lazy evaluation, and hardware acceleration, data scientists and engineers can tackle datasets from gigabytes to petabytes without leaving the Python environment.\n",
    "\n",
    "Unlike R — which often requires external systems (Spark, databases) to scale — Python natively supports scalable computing through libraries designed for performance and distribution. With the right tools and strategies, Python is not just a prototyping language — it’s a production-grade engine for big data and HPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources\n",
    "\n",
    "1. [Dask Documentation](https://docs.dask.org/) — Scalable analytics in Python.\n",
    "2. [RAPIDS.ai](https://rapids.ai/) — GPU Data Science.\n",
    "3. [Apache Spark + PySpark Guide](https://spark.apache.org/docs/latest/api/python/)\n",
    "4. [Polars User Guide](https://pola-rs.github.io/polars-book/)\n",
    "5. [Vaex: Out-of-Core DataFrames](https://vaex.io/)\n",
    "6. [DuckDB: Analytical Database](https://duckdb.org/)\n",
    "7. [Ray: Distributed Computing](https://www.ray.io/)\n",
    "8. [High-Performance Python (Book)](https://www.oreilly.com/library/view/high-performance-python/9781449361747/) — Practical techniques for speed and scalability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
